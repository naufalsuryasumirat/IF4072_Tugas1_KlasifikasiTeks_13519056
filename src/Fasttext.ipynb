{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sastrawi\n",
        "!pip install --upgrade gensim\n",
        "# !pip install --upgrade keras\n",
        "# !pip install --upgrade tensorflow\n",
        "!pip install keras_metrics"
      ],
      "metadata": {
        "id": "4lGgxEnryQsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a74fbc-1c90-4e18-b5cc-303c03bef875"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sastrawi in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_metrics in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras_metrics) (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyspSPBoka2M"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "rZtxDtDuybyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a306d5b-519a-42d0-ed9f-46c63fa0ce99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j33xDhDByL8j"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WmOw692vkeMv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ODotfjqB-aYe"
      },
      "outputs": [],
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from keras.models import Sequential\n",
        "from keras_metrics import precision, recall\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow import stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqHXLCV4_AQo"
      },
      "source": [
        "# Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jXG9ccLd_ELO"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train.csv')[['text_a','label']] # drop row numbers column\n",
        "df_train[\"label\"] = df_train[\"label\"].map({\"yes\":1, \"no\":0})\n",
        "df_test = pd.read_csv('test.csv')\n",
        "df_test[\"label\"] = df_test[\"label\"].map({\"yes\":1, \"no\":0})\n",
        "df_validate = pd.read_csv('dev.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FW3erS_bP6"
      },
      "source": [
        "# Stopword Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sh0jOlaO_iuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc144aa-895d-426e-d2d7-07731f6fafe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sastrawi stopwords: 123\n",
            "nltk stopwords: 9380\n",
            "nltk added stopwords: 9395\n"
          ]
        }
      ],
      "source": [
        "# add stopwords\n",
        "add_stopwords = set(StopWordRemoverFactory().get_stop_words())\n",
        "print('sastrawi stopwords:', len(add_stopwords))\n",
        "\n",
        "stopwords_set = set(stopwords.words())\n",
        "print('nltk stopwords:', len(stopwords_set))\n",
        "stopwords_set = stopwords_set.union(add_stopwords)\n",
        "print('nltk added stopwords:', len(stopwords_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClHSjwic_ocq"
      },
      "source": [
        "# Function to preprocess/normalize text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gl1Chcv7_qmk"
      },
      "outputs": [],
      "source": [
        "stemmer = StemmerFactory().create_stemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HSo7XOkU_sQa"
      },
      "outputs": [],
      "source": [
        "def normalize_text(string: str, stem: bool=True, sw_elim: bool=True) -> List[str]:\n",
        "  # filtering, only characters allowed\n",
        "  filtered = re.sub('[^a-zA-Z]', ' ', string)\n",
        "  # lower-cased and stemmed using Sastrawi\n",
        "  stemmed = stemmer.stem(filtered) if stem else filtered.lower()\n",
        "  # tokenize stemmed string\n",
        "  tokenized = word_tokenize(stemmed)\n",
        "  # eliminate stopwords\n",
        "  res = [word for word in tokenized if word not in stopwords_set] if sw_elim else tokenized\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LsByZLqyL8p"
      },
      "source": [
        "# Proprocess DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WgNEM1GDyL8p"
      },
      "outputs": [],
      "source": [
        "df_train.text_a = df_train.text_a.apply(normalize_text, args=(False, True))\n",
        "df_test.text_a = df_test.text_a.apply(normalize_text, args=(False, True))\n",
        "df_validate.text_a = df_validate.text_a.apply(normalize_text, args=(False, True))\n",
        "\n",
        "df_train = df_train[df_train['text_a'].map(len) > 0]\n",
        "\n",
        "X_train = df_train.text_a\n",
        "y_train = df_train.label\n",
        "X_test = df_test.text_a\n",
        "y_test = df_test.label\n",
        "X_validate = df_validate.text_a\n",
        "y_validate = df_validate.label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Function"
      ],
      "metadata": {
        "id": "dqicOziRG31z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokenized_data, word_vector):\n",
        "    vectorized = []\n",
        "    for sentence in tokenized_data:\n",
        "        if len(sentence) == 0:\n",
        "            continue\n",
        "        sentvec = []\n",
        "        for w in sentence:\n",
        "            if w in word_vector.index_to_key:\n",
        "                sentvec.append(word_vector[w])\n",
        "            else:\n",
        "                sentvec.append(np.zeros((50)))\n",
        "        vectorized.append(sentvec)\n",
        "    return vectorized"
      ],
      "metadata": {
        "id": "ZsTulmbYG9Ut"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(i) for i in X_train])\n",
        "\n",
        "def padding(vec):\n",
        "    counter = 0\n",
        "    padded = []\n",
        "    for i, v in enumerate(vec):\n",
        "        counter += 1\n",
        "        vector = []\n",
        "        if len(v) < max_length:\n",
        "            pad_count = max_length - len(v)\n",
        "            pad = np.zeros((pad_count, 50))\n",
        "            vector = np.append(v, pad, axis=0)\n",
        "        else:\n",
        "            vector = v[:max_length]\n",
        "        padded.append(vector)\n",
        "    return padded"
      ],
      "metadata": {
        "id": "zGGowXcmG9Nf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fasttext"
      ],
      "metadata": {
        "id": "C233xUlhFlkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastText(X_train, vector_size=50, window=6, epochs=30, seed=1)\n",
        "fasttext = model.wv"
      ],
      "metadata": {
        "id": "nfx5qvHuF46c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fasttext = vectorize(X_train, fasttext)\n",
        "X_test_fasttext = vectorize(X_test, fasttext)\n",
        "X_validate_fasttext = vectorize(X_validate, fasttext)"
      ],
      "metadata": {
        "id": "5XSlBn1LGbV-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fasttext = padding(X_train_fasttext)\n",
        "X_test_fasttext = padding(X_test_fasttext)\n",
        "X_validate_fasttext = padding(X_validate_fasttext)"
      ],
      "metadata": {
        "id": "7VDSCT6hGbOT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fasttext = np.array(X_train_fasttext)\n",
        "X_test_fasttext = np.array(X_test_fasttext)\n",
        "X_validate_fasttext = np.array(X_validate_fasttext)"
      ],
      "metadata": {
        "id": "Hitx7GvxGbEx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Fasttext"
      ],
      "metadata": {
        "id": "8MU2SloiHKYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fasttext = Sequential([\n",
        "    LSTM(200, input_shape=(X_train_fasttext.shape[1],X_train_fasttext.shape[2]),return_sequences=True),\n",
        "    LSTM(100, activation=\"sigmoid\",return_sequences=True),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "qNQl2DaOHNrz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fasttext.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", precision(), recall()])"
      ],
      "metadata": {
        "id": "zRxYeSlSHOBw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fasttext.fit(X_train_fasttext, y_train)"
      ],
      "metadata": {
        "id": "gLsN-jAfHN5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2f6768-62a1-4244-f8b1-bf9d0b884269"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "/usr/local/lib/python3.7/dist-packages/keras_metrics/metrics.py:26: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return object.__getattribute__(self, name)\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "675/675 [==============================] - 927s 1s/step - loss: 0.4715 - accuracy: 0.7987 - precision: 0.3239 - recall: 0.8510\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbacae9590>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fasttext.evaluate(X_test_fasttext,y_test)"
      ],
      "metadata": {
        "id": "di5BfOMiHWxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46645254-b752-4816-b391-adbdc46e97ed"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/88 [==============================] - 36s 399ms/step - loss: 0.4279 - accuracy: 0.8404 - precision: 0.3302 - recall: 0.8186\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4278566539287567,\n",
              " 0.8403874039649963,\n",
              " 0.3302321434020996,\n",
              " 0.8185580968856812]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}